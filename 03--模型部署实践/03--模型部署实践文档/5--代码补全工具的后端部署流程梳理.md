# 5--代码补全工具后端服务的部署流程梳理



**目前后端的主要组件：**

- TorchServe 的模型部署
  - 开放端口、挂载地址；
- Flask 后端框架及Redis 数据库的部署
  - 开放端口、挂在地址
  - Redis 数据库的持久化问题及数据转移、导入的验证；



**目前服务部署上的实践方案：**

- 验证模型方案：

  目前的TorchServe模型推理模块与Flask后端及Redis数据库模块合并放在同一个容器内进行服务的部署，验证目前方案的可行性；

- 正式服务部署方案：

  此时需要考虑分开部署，可能需要分别部署在不同的机器上以保证服务的稳定性及提高容错率，分别用不同的容器进行部署也将有利于简化部署困难度，一旦一方故障，则恢复服务的操作也简单且其他服务不受影响。



## 01、TorchServe 的模型部署

**镜像名称：**

- **gameai/torchserver:0.1**
- hub.2980.com/dockerhub/yangdafu/torchserve-cu116:gpu
- hub.2980.com/gameai_test/torchserve:latest



**1、部署指令**

```shell
$ docker run -it --gpus '"device=2"' -p 8889:8888 -p 7000-7002:7000-7002 -v /mnt/cephfs/workspace/codegen-350M:/home/workspace/codegen-350M gameai/torchserver:0.1  /bin/bash
```

```shell
$ docker run --rm -it --gpus '"device=3"' -p 8890:8888 -p 7003-7005:7003-7005 -p 5001:5000  -v /mnt/cephfs/workspace/codegen-6B:/home/workspace/codegen-6B gameai/torchserver:0.1  /bin/bash
```

```shell
$ docker run  --rm --shm-size=1g  --ulimit memlock=-1  --ulimit stack=67108864 -it --gpus all  -p 8889:8888 -p 7000-7002:7000-7002 -p 5000:5000  -v /mnt/cephfs/workspace/codegen-350M:/home/workspace/codegen-350M gameai/torchserver:0.1  /bin/bash
```

Install jupyterlab:

```shell
$ pip install jupyterlab
```

启动jupyter lab:

```shell
$ jupyter lab
```

- Container_id
- token

```shell
$ jupyter server list
```

- jupyter lab的登录地址：

  http://10.17.68.105:8889

**2、启动模型并进行相关测试**

更新config.properties：

```properties
enable_envvars_config=true
inference_address=http://0.0.0.0:7000
management_address=http://0.0.0.0:7001
metrics_address=http://0.0.0.0:7002

job_queue_size=1000
num_of_netty_threads=1
netty_client_threads=1
MKL_NUM_THREADS=20

models={\
	"codegen":{\
	 "1.0": {\
		"defaultVersion": true,\
		"marName": "codegen.mar",\
		"minWorkers": 1,\
		"maxWorkers": 1,\
		"batchSize": 64,\
		"maxBatchDelay": 200,\
		"responseTimeout": 120\
	}\
  }\
}
```



创建MAR模型：

```shell
$ torch-model-archiver --model-name codegen --version 1.0  --serialized-file Transformer_model/pytorch_model.bin  --handler  ./Transformer_handler_generalized.py  --extra-files  "Transformer_model/config.json, ./setup_config.json"
```

运行MAR可执行文件：

```shell
$ torchserve --satrt --model-store model-store  --models codegen.mar  --ts-config  ./config.properties
```

测试部署模型的设置参数是否修改成功：

```shell
$ curl  http://10.17.68.105:7001/models/codegen
```

测试模型推理是否正常：

```shell
$ curl  -X POST  http://10.17.68.105:7000/predictions/codegen  -H  'Content-Type:application/json'  -d '{"data":"import pandas as"}'
```





## 02、Flask 后端框架及Redis 数据库的部署

**预备条件：**

```shell
$ pip install flask
$ pip install flask_restful
$ pip install redis -i http://apt.2980.com/pypi/simple
$ pip install redis-server -i http://apt.2980.com/pypi/simple
```



**1、Redis 数据库在内网的启动指令**

```python
import redis_server
!$redis_server.REDIS_SERVER_PATH  --daemonize  yes
```



 **2、部署脚本**

```python
from flask import Flask
from flask import request
import json
import requests
import redis

# 获取redis数据库连接
r = redis.StrictRedis(host='localhost', port=6379, db=0)

# TorchServe的推理地址
torchserve_url = 'http://0.0.0.0:7000/predictions/codegen'
headers = {'Content-Type':'application/json'}

app = Flask(__name__)

@app.route("/", methods=["POST"])
def query():
    content = request.json
    codehint = content['data']
    # 查询 Redis数据库
    if r.exists(codehint):
        predict = r.get(codehint)
        return predict
    # 转发请求，TorchServe进行推理
    else:
        dict = {}
        dict['data'] = codehint
        # 转发请求，使用Torchserve进行模型推理
        response = requests.post(torchserve_url, data=json.dumps(dict), headers=headers)
        predict = response.text
        # 将结果作为键值对的形式存储到Redis
        r.set(codehint, predict)
        return predict
  

@app.route("update", methods=["POST"])
def update():
    content = request.json
    key = content['codehint']
    value = content['predict']
    r.set(key, value)
    return json.dumps({"success":True}), 200, {"Content-Type":"application/json"}

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5000)
```

<span style='color:brown'>**说明：**</span>

- 在目前的测试原型中，由于Redis数据库是按默认设置到Localhost地址，所以容器在开启时并不需要开启对应的端口给到外部进行访问，只需要开放Flask后端的开放接口给到外部进行访问即可。
- 所以，容器在开始时，需要添加5000端口给Flask作为服务端口：
  - 服务地址：http://10.17.68.105:5000

测试后端服务是否正常运行：

```shell
$ curl -X POST http://0.0.0.0:5000/ -H 'Content-Type:application/json' -d '{"data":"import pandas as"}'

$ curl -X POST http://10.17.68.105:5000/ -H 'Content-Type:application/json' -d '{"data":"import pandas as"}'
```

