# 模型调试的相关问题记录

### 1、修改代码的生成长度

setup_config.json

```json
{
 "model_name":"codegen-350M-mono",
 "mode":"text_generation",
 "do_lower_case":true,
 "num_labels":"0",
 "save_mode":"pretrained",
 "max_length":"256",
 "captum_explanation":true,
 "FasterTransformer":false,
 "embedding_name": "codegen-350M-mono"
}
```



#### <span style='color:brown'>代码细节</span>

目前使用的代码：

```python
def inference(self, input_text):
    """Predict the class (or classes) of the received text using the
        serialized transformers checkpoint.
        Args:
            input_batch (list): List of Text Tensors from the pre-process function is passed here
        Returns:
            list : It returns a list of the predicted value for the input text
        """
    inferences = []
    # Handling inference for text_generation.
    if self.setup_config["mode"] == "text_generation":
        input_text = self.tokenizer(input_text, return_tensors="pt").to(self.device)
        outputs = self.model.generate(**input_text, pad_token_id=self.tokenizer.eos_token_id)
        outputs = self.tokenizer.decode(outputs[0])
        logger.info("Generated text: '%s'", outputs)
        inferences.append(outputs)
        return inferences
```

官方事例给出的代码细节：

```python
# Handling inference for text_generation.
if self.setup_config["mode"] == "text_generation":
    if self.setup_config["model_parallel"]:
        # Need to move the first device, as the trasnformer model has been placed there
        # https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/gpt2/modeling_gpt2.py#L970
        input_ids_batch = input_ids_batch.to("cuda:0")
    outputs = self.model.generate(
        input_ids_batch, max_length=50, do_sample=True, top_p=0.95, top_k=60)
    for i, x in enumerate(outputs):
        inferences.append(
            self.tokenizer.decode(outputs[i], skip_special_tokens=True)
        )
    logger.info("Generated text: '%s'", inferences)
    
print("Generated text", inferences)
return inferences
```

